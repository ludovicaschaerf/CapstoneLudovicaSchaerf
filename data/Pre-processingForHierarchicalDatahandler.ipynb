{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of how to use the datahandler:\n",
    "### the inputs and outputs\n",
    "This notebook contains a very first attempt at multi-label classification.\n",
    "A significant part of the notebook contains the preprocessing steps to turn the\n",
    "raw data (of the csvs, the json and the folder containing the images) into the inputs to the datahandler: **filenames** and **labels**. The results are pickled, thus it is not necessary to rerun this preprocessing on this dataset in the future.\n",
    "The remaining part of the notebook uses the data-generator produced by the datahandler ([datahandler_multilabel.py](./datahandler_multilabel.py)) to train an example of a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the code of the datahandler and of the example model is taken from [this post](https://towardsdatascience.com/multi-label-image-classification-in-tensorflow-2-0-7d4cf8a4bc72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "\n",
    "from datahandler_multilabel import create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the csv of the Tate Dataset to have a list of all the artworks contained in the data folder (which was made using [RetrievingTateModernImages.ipynb](./RetrievingTateModernImages.ipynb)) and the \n",
    "json tree to obtain the target vector of each image. I.e. the paths of the keys \n",
    "to reach the value (the image name) are the values 1 in the target vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ludovica\\Anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (9,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_info = pd.read_csv(os.path.join(\"..\", \"..\", \"Capstone\", \"artwork_data.csv\"), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./results/TateDict.json', 'r') as infile:\n",
    "    tree1 = json.load(infile)\n",
    "len(tree1.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tree1['people']['portraits']['groups'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 2417\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for key in tree1.keys():\n",
    "    for entry in tree1[key].keys():\n",
    "        new_dict[entry] = list()\n",
    "        for values in tree1[key][entry].values():\n",
    "            new_dict[entry] += values\n",
    "    \n",
    "print(len(new_dict.keys()), len(new_dict['portraits']))\n",
    "\n",
    "def getKeysByValue(dictOfElements, valueToFind):\n",
    "    '''Get a list of keys from dictionary which has the given value\n",
    "    '''\n",
    "    listOfKeys = list()\n",
    "    listOfItems = list()\n",
    "    for item in dictOfElements.items():\n",
    "        listOfItems.append(item)\n",
    "    for item in listOfItems:\n",
    "        if valueToFind in item[1]:\n",
    "            listOfKeys.append((valueToFind, item[0]))\n",
    "    \n",
    "    return listOfKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = data_info.accession_number.tolist()\n",
    "tuples = list()\n",
    "\n",
    "for i in range(len(values)):\n",
    "    tuples.append(getKeysByValue(new_dict, values[i]))\n",
    "    \n",
    "print(len(tuples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the key paths to numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2num = {}\n",
    "for i,elt in enumerate(new_dict.keys()):\n",
    "    class2num[elt] = i\n",
    "class2num['portraits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26965,\n",
       " ('A00121',\n",
       "  array([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.])))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_target = []\n",
    "import numpy \n",
    "\n",
    "for tupl in tuples:\n",
    "    if len(tupl) > 0:\n",
    "        zarray = numpy.zeros(147)\n",
    "        for i in range(len(tupl)):\n",
    "            zarray[class2num[tupl[i][1]]] = 1\n",
    "        img_target.append((tupl[0][0], zarray))\n",
    "        \n",
    "len(img_target), img_target[111]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keeping the filenames that are present in the dictectory (that could be downloaded from the links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = os.path.join('..','..','data_tate')\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "labels = []\n",
    "for i,img_targ in enumerate(img_target):\n",
    "    img_targ0 = str(img_targ[0])+'_8.jpg'\n",
    "    if img_targ0 in onlyfiles:\n",
    "        filenames.append(str(img_targ[0])+'_8.jpg')\n",
    "        labels.append(img_targ[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24998\n"
     ]
    }
   ],
   "source": [
    "filenames = [os.path.join('..','..','data_tate',str(filename)) \\\n",
    "                 for filename in filenames]\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dumping the resulting filenames and labels to pickle files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('filenames_level2.pkl', 'wb') as outfile:\n",
    "    pickle.dump(filenames, outfile)\n",
    "    \n",
    "with open('labels_level2.pkl', 'wb') as outfile2:\n",
    "    pickle.dump(labels, outfile2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the pickled inputs to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24998 141\n"
     ]
    }
   ],
   "source": [
    "with open('filenames_level2.pkl', 'rb') as infile:\n",
    "    filenames = pickle.load(infile)\n",
    "    \n",
    "with open('labels_level2.pkl', 'rb') as infile2:\n",
    "    labels = pickle.load(infile2)\n",
    "    \n",
    "print(len(filenames), len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "47\n",
      "113\n",
      "117\n",
      "118\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "def count_class_entries(labels):\n",
    "    classes = [0]*len(labels[1])\n",
    "    for i in range(len(labels[1])):\n",
    "        for label in labels:\n",
    "            if label[i] == 1:\n",
    "                classes[i] += 1\n",
    "                \n",
    "    return classes\n",
    "\n",
    "classes = count_class_entries(labels)\n",
    "for i in range(len(classes)):\n",
    "    if classes[i] < 20:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    }
   ],
   "source": [
    "for i,label in enumerate(labels):\n",
    "    labels[i] = np.delete(label, [34,47,113,117,118,146])\n",
    "\n",
    "print(len(labels[1100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the create_dataset function\n",
    "train_ds = create_dataset(filenames, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#very simple pre-trained model\n",
    "import tensorflow_hub as hub\n",
    "IMG_SIZE = 224\n",
    "CHANNELS = 3\n",
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
    "                                         input_shape=(IMG_SIZE,IMG_SIZE,CHANNELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    feature_extractor_layer,\n",
    "    tf.keras.layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "    tf.keras.layers.Dense(147, activation='sigmoid', name='output')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which we predict positive\n",
    "        \n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5 # Keep it small when transfer learning\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "  loss=tf.keras.losses.binary_crossentropy,\n",
    "  metrics=[macro_f1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds,\n",
    "  epochs=EPOCHS,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
